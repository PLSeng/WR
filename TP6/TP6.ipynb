{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0155d5be",
   "metadata": {},
   "source": [
    "|**Student name:** |PAV Limseng|\n",
    "|---|---|\n",
    "|**Student ID:** |e20211548|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f8b13",
   "metadata": {},
   "source": [
    "# TP6: N-gram & Language Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d0fbd2",
   "metadata": {},
   "source": [
    "## Exercise 1: N-gram Model\n",
    "\n",
    "**N-gram Modeling:** Analyze how additional context improves prediction \\\n",
    "**Corpus:**\n",
    "\"artificial intelligence improves data analysis, artificial intelligence powers modern applications.\" \\\n",
    "**Instructions:** \n",
    "1. Tokenize and preprocess the corpus (lowercase, punctuation as tokens) \n",
    "2. Build a bigram model and a trigram model \n",
    "3. Compute probabilities using Maximum Likelihood Estimation (MLE) \n",
    "4. Predict the next word for the context: \"artificial intelligence\" \n",
    "5. Compare the predictions from: Bigram model, Trigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a959f24",
   "metadata": {},
   "source": [
    "#### 1. Tokenise and preprocess the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286bd922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS: ['artificial', 'intelligence', 'improves', 'data', 'analysis', ',', 'artificial', 'intelligence', 'powers', 'modern', 'applications', '.']\n"
     ]
    }
   ],
   "source": [
    "from TP6_utils import tokenize\n",
    "\n",
    "corpus = \"artificial intelligence improves data analysis, artificial intelligence powers modern applications.\"\n",
    "tokens = tokenize(corpus)\n",
    "\n",
    "print(\"TOKENS:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8f54d",
   "metadata": {},
   "source": [
    "#### 2. Build a bigram model and a trigram model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0190e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bigram_counts = Counter()\n",
    "unigram_counts = Counter()\n",
    "\n",
    "trigram_counts = Counter()\n",
    "bigram_context_counts = Counter()  # counts for (w1, w2) contexts in trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9a73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokens) - 1):\n",
    "    w1, w2 = tokens[i], tokens[i + 1]\n",
    "    unigram_counts[w1] += 1\n",
    "    bigram_counts[(w1, w2)] += 1\n",
    "# count last unigram token too\n",
    "unigram_counts[tokens[-1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6467c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokens) - 2):\n",
    "    w1, w2, w3 = tokens[i], tokens[i + 1], tokens[i + 2]\n",
    "    trigram_counts[(w1, w2, w3)] += 1\n",
    "    bigram_context_counts[(w1, w2)] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0343398",
   "metadata": {},
   "source": [
    "#### 3. Compute probabilities using Maximum Likelihood Estimation (MLE) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d28b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TP6_utils import bigram_mle_prob, trigram_mle_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4fa3cd",
   "metadata": {},
   "source": [
    "#### 4. Predict the next word for the context: \"artificial intelligence\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d6967f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bigram prediction (context = 'intelligence') ---\n",
      "P('improves' | 'intelligence') = 0.5000\n",
      "P('powers' | 'intelligence') = 0.5000\n"
     ]
    }
   ],
   "source": [
    "from TP6_utils import predict_next_bigram, predict_next_trigram\n",
    "\n",
    "context = (\"artificial\", \"intelligence\")\n",
    "\n",
    "# Bigram uses only the last word as context\n",
    "bigram_predictions = predict_next_bigram(\n",
    "    context_word=context[1],\n",
    "    top_k=10,\n",
    "    unigram_counts=unigram_counts,\n",
    "    bigram_counts=bigram_counts,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Bigram prediction (context = 'intelligence') ---\")\n",
    "for w, p in bigram_predictions:\n",
    "    print(f\"P({w!r} | 'intelligence') = {p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77b67f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trigram prediction (context = 'artificial intelligence') ---\n",
      "P('improves' | 'artificial intelligence') = 0.5000\n",
      "P('powers' | 'artificial intelligence') = 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Trigram uses the two-word context\n",
    "trigram_predictions = predict_next_trigram(\n",
    "    w1=context[0],\n",
    "    w2=context[1],\n",
    "    top_k=10,\n",
    "    bigram_context_counts=bigram_context_counts,\n",
    "    trigram_counts=trigram_counts,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Trigram prediction (context = 'artificial intelligence') ---\")\n",
    "for w, p in trigram_predictions:\n",
    "    print(f\"P({w!r} | 'artificial intelligence') = {p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a5691b",
   "metadata": {},
   "source": [
    "#### 5. Compare the predictions from: Bigram model, Trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20f66d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparison Summary ===\n",
      "Bigram top: [('improves', 0.5), ('powers', 0.5)]\n",
      "Trigram top: [('improves', 0.5), ('powers', 0.5)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Comparison Summary ===\")\n",
    "print(\"Bigram top:\", bigram_predictions[:3])\n",
    "print(\"Trigram top:\", trigram_predictions[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cd192d",
   "metadata": {},
   "source": [
    "## Exercise 2: Data Sparsity & Smoothing Techniques: How smoothing changes probability distribution and model behavior\n",
    "\n",
    "**Instructions:**\n",
    "1. Use the corpus: \"students study machine learning. Students study Data Science.\" \n",
    "2. Build a bigram model without smoothing \n",
    "3. Apply Laplace smoothing to the same model \n",
    "4. Use the test sentence: \"students study ai.\" \n",
    "5. Compute: \n",
    "    - Sentence probability without smoothing \n",
    "    - Sentence probability with smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0eb57a",
   "metadata": {},
   "source": [
    "#### 1. Use the corpus: \"students study machine learning. Students study Data Science.\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45a2b69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS 2: ['students', 'study', 'machine', 'learning', '.', 'students', 'study', 'data', 'science', '.']\n"
     ]
    }
   ],
   "source": [
    "corpus2 = \"students study machine learning. Students study Data Science.\"\n",
    "tokens2 = tokenize(corpus2)\n",
    "\n",
    "print(\"TOKENS 2:\", tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e61e28",
   "metadata": {},
   "source": [
    "#### 2. Build a bigram model without smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8cdd210",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts2 = Counter(tokens2)\n",
    "bigram_counts2 = Counter(zip(tokens2[:-1], tokens2[1:]))\n",
    "\n",
    "vocab2 = set(tokens2)\n",
    "vocab2.add(\"ai\") \n",
    "V2 = len(vocab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594f9d5",
   "metadata": {},
   "source": [
    "#### 3. Apply Laplace smoothing to the same model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5cff220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TP6_utils import sentence_prob_no_smoothing, sentence_prob_laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da01f8",
   "metadata": {},
   "source": [
    "#### 4. Use the test sentence: \"students study ai.\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71791869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence probability (no smoothing): 0.0\n",
      "Sentence probability (Laplace smoothing): 0.00375\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"students study ai.\"\n",
    "test_tokens = tokenize(test_sentence)\n",
    "\n",
    "print(\"Sentence probability (no smoothing):\",\n",
    "      sentence_prob_no_smoothing(test_tokens, unigram_counts2, bigram_counts2))\n",
    "\n",
    "print(\"Sentence probability (Laplace smoothing):\",\n",
    "      sentence_prob_laplace(test_tokens, unigram_counts2, bigram_counts2, V2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f9882",
   "metadata": {},
   "source": [
    "## Exercise 3: Evaluate language models using Perplexity \n",
    "\n",
    "**Instructions:** \n",
    "1. Choose one dataset: \n",
    "    - Brown corpus (NLTK), or Wikipedia \n",
    "2. Preprocess the text (tokenization, lowercase) \n",
    "3. Split the dataset: 80% for training, 20% for testing \n",
    "4. Train two bigram models: \n",
    "    - Without smoothing \n",
    "    - With Laplace smoothing \n",
    "5. Compute perplexity on the test set for both models \n",
    "6. Compare the perplexity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27925d42",
   "metadata": {},
   "source": [
    "#### 1. Choose one dataset: Brown corpus (NLTK), or Wikipedia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e74b071e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\limse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8afb38b",
   "metadata": {},
   "source": [
    "#### 2. Preprocess the text (tokenization, lowercase) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1174980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in Brown corpus: 1161194\n"
     ]
    }
   ],
   "source": [
    "tokens3 = [w.lower() for w in brown.words()]\n",
    "tokens3 = [\"<s>\"] + tokens3 + [\"</s>\"]\n",
    "\n",
    "print(\"Total tokens in Brown corpus:\", len(tokens3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b95d3",
   "metadata": {},
   "source": [
    "#### 3. Split the dataset: 80% for training, 20% for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88f11729",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx3 = int(0.8 * len(tokens3))\n",
    "train_tokens3 = tokens3[:split_idx3]\n",
    "test_tokens3 = tokens3[split_idx3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdbe52e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens: 928955\n",
      "Test tokens: 232239\n"
     ]
    }
   ],
   "source": [
    "print(\"Train tokens:\", len(train_tokens3))\n",
    "print(\"Test tokens:\", len(test_tokens3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0445576",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram3 = Counter(train_tokens3)\n",
    "bigram3 = Counter(zip(train_tokens3[:-1], train_tokens3[1:]))\n",
    "\n",
    "vocab3 = set(train_tokens3)\n",
    "V3 = len(vocab3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66b3d1",
   "metadata": {},
   "source": [
    "#### 4. Train two bigram models: \n",
    "    - Without smoothing \n",
    "    - With Laplace smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66c1d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TP6_utils import bigram_mle, bigram_laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce549881",
   "metadata": {},
   "source": [
    "#### 5. Compute perplexity on the test set for both models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddede0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TP6_utils import perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262cb2d",
   "metadata": {},
   "source": [
    "#### 6. Compare the perplexity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "060572da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (Bigram MLE, no smoothing): inf\n",
      "Perplexity (Bigram + Laplace): 4938.761222455625\n"
     ]
    }
   ],
   "source": [
    "pp_mle = perplexity(\n",
    "    test_tokens3,\n",
    "    lambda w1, w2: bigram_mle(w1, w2, unigram3, bigram3)\n",
    ")\n",
    "pp_laplace = perplexity(\n",
    "    test_tokens3,\n",
    "    lambda w1, w2: (bigram3[(w1, w2)] + 1) / (unigram3[w1] + V3)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Perplexity (Bigram MLE, no smoothing):\", pp_mle)\n",
    "print(\"Perplexity (Bigram + Laplace):\", pp_laplace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
