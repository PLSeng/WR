{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student:** PAV Limseng\n",
    "\n",
    "## TP5: Word Embedings\n",
    "### 1. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) install require libraries(gensim, nltk, sklearn, potly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.4.0)\n",
      "Requirement already satisfied: nltk in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: plotly in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (6.5.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from gensim) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: click in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk) (2025.10.23)\n",
      "Requirement already satisfied: tqdm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from plotly) (2.13.0)\n",
      "Requirement already satisfied: packaging in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from plotly) (25.0)\n",
      "Requirement already satisfied: wrapt in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim nltk scikit-learn plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Download NLTK Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Load Brown sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. .\n",
      "`` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
      "The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "sentences = brown.sents()\n",
    "\n",
    "for i in range(5):\n",
    "    print(' '.join(sentences[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) preprocess: lowercase words, remove punctuation, keep non-empty sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fulton county grand jury said friday an investigation of atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "the jury further said in term-end presentments that the city executive committee , which had over-all charge of the election , `` deserves the praise and thanks of the city of atlanta '' for the manner in which the election was conducted .\n",
      "the september-october term jury had been charged by fulton superior court judge durwood pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by mayor-nominate ivan allen jr. .\n",
      "`` only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
      "the jury said it did find that many of georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n"
     ]
    }
   ],
   "source": [
    "# lowercase all words in the sentences\n",
    "lowercase_sentences = [[word.lower() for word in sentence] for sentence in sentences]\n",
    "\n",
    "for i in range(5):\n",
    "    print(' '.join(lowercase_sentences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fulton county grand jury said friday an investigation of atlanta's recent primary election produced `` no evidence '' that any irregularities took place\n",
      "the jury further said in term-end presentments that the city executive committee which had over-all charge of the election `` deserves the praise and thanks of the city of atlanta '' for the manner in which the election was conducted\n",
      "the september-october term jury had been charged by fulton superior court judge durwood pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by mayor-nominate ivan allen jr.\n",
      "`` only a relative handful of such reports was received '' the jury said `` considering the widespread interest in the election the number of voters and the size of this city ''\n",
      "the jury said it did find that many of georgia's registration and election laws `` are outmoded or inadequate and often ambiguous ''\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation from the sentences\n",
    "import string\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "cleaned_sentences = [[word for word in sentence if word not in punctuation] for sentence in lowercase_sentences]\n",
    "\n",
    "for i in range(5):\n",
    "    print(' '.join(cleaned_sentences[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) train a Word2Vec model using: vector_size = 100, window = 5, min_count = 5, sg = 1 (Skip-gram), epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to 'government':\n",
      "jurisdiction: 0.7775\n",
      "federal: 0.7619\n",
      "india: 0.7612\n",
      "legislation: 0.7529\n",
      "export-import: 0.7476\n"
     ]
    }
   ],
   "source": [
    "# train a Word2Vec model using: vector_size = 100, window = 5, min_count = 5, sg = 1 (Skip-gram), epoch = 10\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=cleaned_sentences, vector_size=100, window=5, min_count=5, sg=1, epochs=10)\n",
    "\n",
    "# model.save(\"brown_word2vec_gensim.model\")\n",
    "# print(\"Word2Vec model trained and saved as 'brown_word2vec_gensim.model'\")\n",
    "\n",
    "# test the model by finding the top 5 most similar words to 'government'\n",
    "similar_words = model.wv.most_similar('government', topn=5)\n",
    "print(\"Top 5 words similar to 'government':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Print the vector for \"king\" (first 10 value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'king' (first 10 values):\n",
      "[-0.15182608  0.18260102 -0.11122134  0.10824263 -0.28236368 -0.2302311\n",
      " -0.04883349  0.5273391  -0.42286196  0.10130822]\n"
     ]
    }
   ],
   "source": [
    "king_vector = model.wv['king']\n",
    "\n",
    "print(\"Vector for 'king' (first 10 values):\")\n",
    "print(king_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) show 5 most similar words to \"woman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to 'woman':\n",
      "girl: 0.8022\n",
      "lean: 0.7405\n",
      "lonely: 0.7052\n",
      "henrietta: 0.7038\n",
      "lady: 0.6996\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar('woman', topn=5)\n",
    "print(\"Top 5 words similar to 'woman':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Perform Analogy: king - man + woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analogy result vector (first 10 values):\n",
      "[-0.4602321   0.24973573  0.09206221 -0.3104469  -0.6452747   0.53791344\n",
      " -0.2224539   0.44033635 -0.63946295 -0.10849126]\n"
     ]
    }
   ],
   "source": [
    "analogy_result = king_vector - model.wv['man'] + model.wv['woman']\n",
    "\n",
    "print('analogy result vector (first 10 values):')\n",
    "print(analogy_result[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Check is \"Government\" is in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(model.wv.index_to_key)\n",
    "\n",
    "'government' in vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Print the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 14209\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Select sample word list for visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample words for visualization: ['government', 'politics', 'election', 'president', 'country', 'king', 'queen', 'man', 'woman']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_words = ['government', 'politics', 'election', 'president', 'country', \n",
    "                'king', 'queen', 'man', 'woman']\n",
    "sample_vectors = [model.wv[word] for word in sample_words if word in vocab]\n",
    "sample_vectors = np.array(sample_vectors)\n",
    "\n",
    "print(\"Sample words for visualization:\", sample_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Filter words that exist in the Word2Vec vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered words for visualization: ['government', 'politics', 'election', 'president', 'country', 'king', 'queen', 'man', 'woman']\n"
     ]
    }
   ],
   "source": [
    "filtered_words = [word for word in sample_words if word in vocab]\n",
    "filtered_vectors = np.array([model.wv[word] for word in filtered_words])\n",
    "\n",
    "print(\"Filtered words for visualization:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) Apply PCA to reduce vectors to 2D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced vectors shape: (9, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(filtered_vectors)\n",
    "\n",
    "print(\"Reduced vectors shape:\", reduced_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14) Create a Plotly scatter plot with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text+x+y",
         "mode": "markers+text",
         "text": [
          "government",
          "politics",
          "election",
          "president",
          "country",
          "king",
          "queen",
          "man",
          "woman"
         ],
         "textposition": "top center",
         "type": "scatter",
         "x": {
          "bdata": "5W/YP0Z1rT4/RaU/QmKBP4gVtTyEy6y60hhUvm6s+78ndwvA",
          "dtype": "f4"
         },
         "y": {
          "bdata": "kJUnvz8uh7+0mKo+PX3jP7ch9b+FolU/hK+2PgHWkz4OlRA9",
          "dtype": "f4"
         }
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "2D PCA of Word Vectors"
        },
        "xaxis": {
         "title": {
          "text": "PC1"
         }
        },
        "yaxis": {
         "title": {
          "text": "PC2"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers+text',\n",
    "    text=filtered_words,\n",
    "    textposition='top center',\n",
    "    hoverinfo='text + x + y',\n",
    "))\n",
    "fig.update_layout(title='2D PCA of Word Vectors', xaxis_title='PC1', yaxis_title='PC2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15) Print the first 500 characters of the Plotly JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"hoverinfo\": \"text+x+y\",\n",
      "            \"mode\": \"markers+text\",\n",
      "            \"text\": [\n",
      "                \"government\",\n",
      "                \"politics\",\n",
      "                \"election\",\n",
      "                \"president\",\n",
      "                \"country\",\n",
      "                \"king\",\n",
      "                \"queen\",\n",
      "                \"man\",\n",
      "                \"woman\"\n",
      "            ],\n",
      "            \"textposition\": \"top center\",\n",
      "            \"x\": {\n",
      "                \"dtype\": \"f4\",\n",
      "                \"bdata\": \"5W/YP0Z1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "fig_json = fig.to_json()\n",
    "fig_dict = json.loads(fig_json)\n",
    "pretty_json = json.dumps(fig_dict, indent=4)\n",
    "print(pretty_json[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Prepare Corpus: Collect documents, lowercase, remove punctuation, tokenize. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Prepare Corpus: Collect documents, lowercase, remove punctuation, tokenize. \n",
    "2) Compute TF: Count word frequency per document; normalize by total words. \n",
    "3) Compute IDF: Count in how many documents each word appears. \n",
    "4) Compute TF-IDF: Multiply TF × IDF for each term in each document. \n",
    "5) Create TF-IDF Matrix: Rows = documents, Columns = terms, values = TF-IDF \n",
    "scores. \n",
    "6) Explore Results: High TF-IDF → important/unique words; Low TF-IDF → \n",
    "common words. \n",
    "7) Dataset: https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 sentences from the Brown corpus:\n",
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. .\n",
      "`` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
      "The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n",
      "\n",
      "Lowercasing all words...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 lowercase sentences:\n",
      "the fulton county grand jury said friday an investigation of atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "the jury further said in term-end presentments that the city executive committee , which had over-all charge of the election , `` deserves the praise and thanks of the city of atlanta '' for the manner in which the election was conducted .\n",
      "the september-october term jury had been charged by fulton superior court judge durwood pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by mayor-nominate ivan allen jr. .\n",
      "`` only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
      "the jury said it did find that many of georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n",
      "\n",
      "Removing punctuation...\n",
      "First 5 cleaned sentences:\n",
      "the fulton county grand jury said friday an investigation of atlanta's recent primary election produced no evidence that any irregularities took place\n",
      "the jury further said in term-end presentments that the city executive committee which had over-all charge of the election deserves the praise and thanks of the city of atlanta for the manner in which the election was conducted\n",
      "the september-october term jury had been charged by fulton superior court judge durwood pye to investigate reports of possible irregularities in the hard-fought primary which was won by mayor-nominate ivan allen jr.\n",
      "only a relative handful of such reports was received the jury said considering the widespread interest in the election the number of voters and the size of this city\n",
      "the jury said it did find that many of georgia's registration and election laws are outmoded or inadequate and often ambiguous\n",
      "\n",
      "Removing stopwords...\n",
      "\n",
      "First 5 cleaned sentences after removing stopwords:\n",
      "fulton county grand jury said friday investigation atlanta's recent primary election produced evidence irregularities took place\n",
      "jury said term-end presentments city executive committee over-all charge election deserves praise thanks city atlanta manner election conducted\n",
      "september-october term jury charged fulton superior court judge durwood pye investigate reports possible irregularities hard-fought primary mayor-nominate ivan allen jr.\n",
      "relative handful reports received jury said considering widespread interest election number voters size city\n",
      "jury said find many georgia's registration election laws outmoded inadequate often ambiguous\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('brown')\n",
    "import string\n",
    "from nltk.corpus import brown, stopwords\n",
    "\n",
    "# test corpus\n",
    "sentences = brown.sents()\n",
    "print(\"First 5 sentences from the Brown corpus:\")\n",
    "for i in range(5):\n",
    "    print(' '.join(sentences[i]))\n",
    "\n",
    "print(\"\\nLowercasing all words...\")\n",
    "lowercase_sentences = [[word.lower() for word in sentence] for sentence in sentences]\n",
    "print(\"First 5 lowercase sentences:\")\n",
    "for i in range(5):\n",
    "    print(' '.join(lowercase_sentences[i]))\n",
    "\n",
    "print(\"\\nRemoving punctuation...\")\n",
    "punctuation = set(string.punctuation)\n",
    "punctuation.add(\"``\")\n",
    "punctuation.add(\"''\")\n",
    "cleaned_sentences = [[word for word in sentence if word not in punctuation] for sentence in lowercase_sentences]\n",
    "print(\"First 5 cleaned sentences:\")\n",
    "for i in range(5):\n",
    "    print(' '.join(cleaned_sentences[i]))\n",
    "    \n",
    "# remove stopwords (Optional: Remove if not needed)\n",
    "print(\"\\nRemoving stopwords...\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "cleaned_sentences = [[word for word in sentence if word not in stop_words] for sentence in cleaned_sentences]\n",
    "print(\"\\nFirst 5 cleaned sentences after removing stopwords:\")\n",
    "for i in range(5):\n",
    "    print(' '.join(cleaned_sentences[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Compute TF: Count word frequency per document; normalize by total words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(documents):\n",
    "    tf_list = []\n",
    "    for doc in documents:\n",
    "        tf_dict = {}\n",
    "        total_words = len(doc)\n",
    "        for word in doc:\n",
    "            tf_dict[word] = tf_dict.get(word, 0) + 1\n",
    "        for word in tf_dict:\n",
    "            tf_dict[word] /= total_words\n",
    "        tf_list.append(tf_dict)\n",
    "    return tf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF for first document:\n",
      "{'fulton': 0.0625, 'county': 0.0625, 'grand': 0.0625, 'jury': 0.0625, 'said': 0.0625, 'friday': 0.0625, 'investigation': 0.0625, \"atlanta's\": 0.0625, 'recent': 0.0625, 'primary': 0.0625, 'election': 0.0625, 'produced': 0.0625, 'evidence': 0.0625, 'irregularities': 0.0625, 'took': 0.0625, 'place': 0.0625}\n"
     ]
    }
   ],
   "source": [
    "# test function\n",
    "tf_documents = compute_tf(cleaned_sentences)\n",
    "print(\"\\nTF for first document:\")\n",
    "print(tf_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Compute IDF: Count in how many documents each word appears. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_idf(documents):\n",
    "    idf_dict = {}\n",
    "    total_documents = len(documents)\n",
    "    for doc in documents:\n",
    "        unique_words = set(doc)\n",
    "        for word in unique_words:\n",
    "            idf_dict[word] = idf_dict.get(word, 0) + 1\n",
    "    for word in idf_dict:\n",
    "        idf_dict[word] = math.log(total_documents / (idf_dict[word] + 1)) + 1\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IDF for some sample words:\n",
      "government: 6.0087\n",
      "politics: 7.7083\n",
      "election: 7.6393\n",
      "president: 6.0514\n",
      "country: 6.2010\n"
     ]
    }
   ],
   "source": [
    "# test function\n",
    "idf_dict = compute_idf(cleaned_sentences)\n",
    "print(\"\\nIDF for some sample words:\")\n",
    "sample_words = ['government', 'politics', 'election', 'president', 'country']\n",
    "for word in sample_words:\n",
    "    print(f\"{word}: {idf_dict.get(word, 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Compute TF-IDF: Multiply TF × IDF for each term in each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(tf_documents, idf_dict):\n",
    "    tfidf_documents = []\n",
    "    for tf_dict in tf_documents:\n",
    "        tfidf_dict = {}\n",
    "        for word, tf_value in tf_dict.items():\n",
    "            idf_value = idf_dict.get(word, 0)\n",
    "            tfidf_dict[word] = tf_value * idf_value\n",
    "        tfidf_documents.append(tfidf_dict)\n",
    "    return tfidf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF for first document:\n",
      "{'fulton': 0.5740103135748488, 'county': 0.435395072041164, 'grand': 0.5053470455330918, 'jury': 0.48450382000289965, 'said': 0.27408000566163065, 'friday': 0.4924510184707276, 'investigation': 0.5003443763034958, \"atlanta's\": 0.6467072391877038, 'recent': 0.42343563682157975, 'primary': 0.46137767255837364, 'election': 0.47745410161881563, 'produced': 0.4667573356065763, 'evidence': 0.41584055196114283, 'irregularities': 0.6099705726313214, 'took': 0.3696325566096862, 'place': 0.35135621799620637}\n"
     ]
    }
   ],
   "source": [
    "# tf_documents = compute_tf(cleaned_sentences)\n",
    "# idf_dict = compute_idf(cleaned_sentences)\n",
    "tfidf_documents = compute_tfidf(tf_documents, idf_dict) \n",
    "\n",
    "print(\"\\nTF-IDF for first document:\")\n",
    "print(tfidf_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Create TF-IDF Matrix: Rows = documents, Columns = terms, values = TF-IDF scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_matrix(tfidf_documents):\n",
    "    all_terms = set()\n",
    "    for doc in tfidf_documents:\n",
    "        all_terms.update(doc.keys())\n",
    "    all_terms = sorted(list(all_terms))\n",
    "    \n",
    "    term_index = {term: idx for idx, term in enumerate(all_terms)}\n",
    "    tfidf_matrix = np.zeros((len(tfidf_documents), len(all_terms)))\n",
    "    \n",
    "    for doc_idx, doc in enumerate(tfidf_documents):\n",
    "        for term, score in doc.items():\n",
    "            term_idx = term_index[term]\n",
    "            tfidf_matrix[doc_idx, term_idx] = score\n",
    "            \n",
    "    return tfidf_matrix, all_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF matrix shape: (57340, 49619)\n",
      "Number of unique terms: 49619\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix, all_terms = create_tfidf_matrix(tfidf_documents)\n",
    "print(\"\\nTF-IDF matrix shape:\", tfidf_matrix.shape)\n",
    "print(\"Number of unique terms:\", len(all_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Explore Results: High TF-IDF → important/unique words; Low TF-IDF → common words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 terms with highest TF-IDF in the first document:\n",
      "atlanta's: 0.6467\n",
      "irregularities: 0.6100\n",
      "fulton: 0.5740\n",
      "grand: 0.5053\n",
      "investigation: 0.5003\n"
     ]
    }
   ],
   "source": [
    "# For demonstration, print top 5 terms with highest TF-IDF in the first document\n",
    "first_doc_tfidf = tfidf_documents[0]\n",
    "sorted_terms = sorted(first_doc_tfidf.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop 5 terms with highest TF-IDF in the first document:\")\n",
    "for term, score in sorted_terms[:5]:\n",
    "    print(f\"{term}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Dataset: https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 70.6kB [00:00, 480kB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 84.2kB [00:00, 344kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed.\n",
      "\n",
      "Extracting dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract: 100%|████████████████████████████████| 12/12 [00:00<00:00, 2453.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed.\n",
      "\n",
      "Removing macOS hidden files (.DS_Store, __MACOSX)...\n",
      "Removed directory: data/__MACOSX\n",
      "Removed file: data/sentiment labelled sentences/.DS_Store\n",
      "macOS hidden files removed.\n",
      "\n",
      "Removing zip file...\n",
      "Zip file removed.\n",
      "\n",
      "Organizing extracted files...\n",
      "Organization completed.\n",
      "\n",
      "Dataset is ready in the 'data' folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "link = 'https://archive.ics.uci.edu/static/public/331/sentiment+labelled+sentences.zip'\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "zip_path = 'data/sentiment_labelled_sentences.zip'\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "\n",
    "    # stream download with progress bar\n",
    "    response = requests.get(link, stream=True)\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "    block_size = 1024\n",
    "\n",
    "    with open(zip_path, 'wb') as f, tqdm(\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        desc='Download',\n",
    "        ncols=80\n",
    "    ) as progress:\n",
    "        for chunk in response.iter_content(block_size):\n",
    "            if chunk:  # filter out keep-alive chunks\n",
    "                f.write(chunk)\n",
    "                progress.update(len(chunk))\n",
    "\n",
    "    print(\"Download completed.\\n\")\n",
    "\n",
    "    print(\"Extracting dataset...\")\n",
    "\n",
    "    # extract with progress bar\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()\n",
    "        with tqdm(\n",
    "            total=len(file_list),\n",
    "            desc='Extract',\n",
    "            ncols=80\n",
    "        ) as progress:\n",
    "            for member in file_list:\n",
    "                zip_ref.extract(member, 'data')\n",
    "                progress.update(1)\n",
    "\n",
    "    print(\"Extraction completed.\\n\")\n",
    "\n",
    "    # remove macOS hidden files if any\n",
    "    print(\"Removing macOS hidden files (.DS_Store, __MACOSX)...\")\n",
    "\n",
    "    for root, dirs, files in os.walk('data'):\n",
    "        # remove __MACOSX directories\n",
    "        if '__MACOSX' in dirs:\n",
    "            macos_dir = os.path.join(root, '__MACOSX')\n",
    "            try:\n",
    "                shutil.rmtree(macos_dir)\n",
    "                print(f\"Removed directory: {macos_dir}\")\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "        # remove any .DS_Store files\n",
    "        if '.DS_Store' in files:\n",
    "            ds_path = os.path.join(root, '.DS_Store')\n",
    "            try:\n",
    "                os.remove(ds_path)\n",
    "                print(f\"Removed file: {ds_path}\")\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "    print(\"macOS hidden files removed.\\n\")\n",
    "\n",
    "    # remove zip file\n",
    "    print(\"Removing zip file...\")\n",
    "    try:\n",
    "        os.remove(zip_path)\n",
    "        print(\"Zip file removed.\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Zip file already removed.\\n\")\n",
    "\n",
    "    # move extracted files to data directory\n",
    "    extracted_folder = 'data/sentiment labelled sentences'\n",
    "    if os.path.exists(extracted_folder):\n",
    "        print(\"Organizing extracted files...\")\n",
    "        for file_name in os.listdir(extracted_folder):\n",
    "            full_file_name = os.path.join(extracted_folder, file_name)\n",
    "            if os.path.isfile(full_file_name):\n",
    "                shutil.move(full_file_name, os.path.join('data', file_name))\n",
    "        # remove extracted folder\n",
    "        shutil.rmtree(extracted_folder)\n",
    "        print(\"Organization completed.\\n\")\n",
    "\n",
    "print(\"Dataset is ready in the 'data' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_file = ['yelp_labelled.txt', 'amazon_cells_labelled.txt', 'imdb_labelled.txt']\n",
      "Number of sentences loaded: 3000\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "data_files = os.listdir('data/')\n",
    "data_files.remove('readme.txt')\n",
    "print(f'data_file = {data_files}')\n",
    "\n",
    "# read(only texts not labels) and combine all the text files to do tf-idf \n",
    "texts = []\n",
    "for file in data_files:\n",
    "    file_path = os.path.join('data', file)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentence, label = line.strip().split('\\t')\n",
    "            texts.append(sentence)\n",
    "print(f'Number of sentences loaded: {len(texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 sentences:\n",
      "Wow... Loved this place.\n",
      "Crust is not good.\n",
      "Not tasty and the texture was just nasty.\n",
      "Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\n",
      "The selection on the menu was great and so were the prices.\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 sentences:\")\n",
    "for i in range(5):\n",
    "    print(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing texts: lowercasing, removing stopwords, punctuation, stemming, lemmatization, tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████████████████| 3000/3000 [00:00<00:00, 6652.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 tokenized sentences:\n",
      "['wow', 'love', 'place']\n",
      "['crust', 'good']\n",
      "['tasti', 'textur', 'nasti']\n",
      "['stop', 'late', 'may', 'bank', 'holiday', 'rick', 'steve', 'recommend', 'love', 'it']\n",
      "['select', 'menu', 'great', 'price']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare Corpus: Collect documents, lowercase, remove punctuation, tokenize. \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_texts = []\n",
    "print(\"\\nPreprocessing texts: lowercasing, removing stopwords, punctuation, stemming, lemmatization, tokenization...\")\n",
    "with tqdm(total=len(texts), desc='Preprocessing', ncols=80) as progress:\n",
    "    for sentence in texts:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = ' '.join([word for word in sentence.split() if word not in stop_words])\n",
    "        sentence = ''.join([char for char in sentence if char not in string.punctuation])\n",
    "        # stemming and lemmatization\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        sentence = ' '.join([stemmer.stem(word) for word in sentence.split()])\n",
    "        sentence = ' '.join([lemmatizer.lemmatize(word) for word in sentence.split()])\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tokenized_texts.append(tokens)\n",
    "        progress.update(1)\n",
    "    \n",
    "print(\"First 5 tokenized sentences:\")\n",
    "for i in range(5):\n",
    "    print(tokenized_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF Calculation: 100%|█████████████████████████| 3/3 [00:00<00:00, 19.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF for first document:\n",
      "{'wow': 2.353485806198311, 'love': 1.4912560248323303, 'place': 1.3980610707592764}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate TF-IDF matrix\n",
    "with tqdm(total=3, desc='TF-IDF Calculation', ncols=80) as progress:\n",
    "    tf_documents = compute_tf(tokenized_texts)\n",
    "    progress.update(1)\n",
    "    idf_dict = compute_idf(tokenized_texts)\n",
    "    progress.update(1)\n",
    "    tfidf_documents = compute_tfidf(tf_documents, idf_dict)\n",
    "    progress.update(1)\n",
    "# print some sample TF-IDF values\n",
    "print(\"\\nTF-IDF for first document:\")\n",
    "print(tfidf_documents[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
